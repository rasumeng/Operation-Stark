{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943194f9",
   "metadata": {},
   "source": [
    "# üîπ 9-Week Data Scientist Roadmap (Day-by-Day Tasks + Resources)\n",
    "\n",
    "## Weekly Rhythm\n",
    "- Classes: Tuesday & Thursday ‚Üí mostly focus on class, optional 15‚Äì20 min micro-review\n",
    "- Free Days: Monday, Wednesday ‚Üí main ML / coding / stats sessions (~2‚Äì3 hrs)\n",
    "- Break / Recharge: Friday ‚Üí rest\n",
    "- Busy / Church: Saturday ‚Üí optional micro-task 15 min if energy\n",
    "- Sunday: Optional reflection / portfolio notes 30‚Äì60 min\n",
    "\n",
    "---\n",
    "## Phase 1 ‚Äì Week 1‚Äì3: ML Foundations + Stats + Why It Works\n",
    "### Week 1 (Day 1‚Äì7) ‚Äì Linear Regression + Stats\n",
    "Tasks: \n",
    "- [x] Install Python packages: numpy, pandas, matplotlib, scikit-learn\n",
    "- [ ] Load dataset (Titanic / Housing)\n",
    "- [ ] Explore dataset: missing values, column types, basic stats\n",
    "- [ ] Train linear regression, print coefficients\n",
    "- [ ] Plot predicted vs actual values\n",
    "\n",
    "Reflection: \n",
    "- What does each coefficient mean?\n",
    "- How does partial derivitave connect to weight updates?\n",
    "\n",
    "Resource: [Linear Regression in Python](https://realpython.com/linear-regression-in-python/)\n",
    "\n",
    "### Week 2 (Day 8‚Äì14) ‚Äì Logistic Regression + Model Evaluation\n",
    "Tasks: \n",
    "- [ ] Train logistic regression on binary classification\n",
    "- [ ] Split train/test sets\n",
    "- [ ] Evaluate: Accuracy, Precision, Recall, F1\n",
    "- [ ] Optional: Overfit a decision tree to observe effects\n",
    "\n",
    "Reflection: \n",
    "- Why does overfitting occur?\n",
    "- How does model complexity impact bias vs variance?\n",
    "\n",
    "Resource: [Linear Regression in Python](https://realpython.com/linear-regression-in-python/)\n",
    "\n",
    "### Week 3 (Day 15‚Äì21) ‚Äì Decision Trees + Feature Engineering + Gradient Descent Intuition\n",
    "Tasks: \n",
    "- [ ] Train decision tree classifier\n",
    "- [ ] Compare with logistic regression\n",
    "- [ ] Engineer simple features: combine columns, normalize/scale\n",
    "- [ ] Optional: simple gradient descent manually on small linear regression\n",
    "\n",
    "Reflection: \n",
    "- Which features mattered most?\n",
    "- How does math explain weight updates?\n",
    "\n",
    "Resource: [Decision Tree Classification in Python](https://realpython.com/decision-tree-classification-python/)\n",
    "\n",
    "---\n",
    "## Phase 2 ‚Äì Week 4‚Äì6: Real Projects + Feature Engineering\n",
    "### Week 4 (Day 22‚Äì28) ‚Äì Dataset + Baseline Model\n",
    "Tasks: \n",
    "- [ ] Pick real dataset (Kaggle / UCI)\n",
    "- [ ] Clean dataset: missing values, categorical encoding, scaling\n",
    "- [ ] Train baseline models: Linear / Logistic regression\n",
    "- [ ] Analyze basic feature importance / correlation\n",
    "\n",
    "Reflection: \n",
    "- What does each coefficient mean?Which features are strongest predictors?\n",
    "- How does feature variance affect predictions?\n",
    "\n",
    "Resource: [Pandas & Dataset Cleaning](https://www.kaggle.com/learn/pandas)\n",
    "\n",
    "### Week 5 (Day 29‚Äì35) ‚Äì Feature Engineering + Model Comparison\n",
    "Tasks: \n",
    "- [ ] Engineer new features (interactions, ratios, polynomial features)\n",
    "- [ ] Train multiple models: Decision Trees, Random Forest, basic Gradient Boosting\n",
    "- [ ] Evaluate metrics: Accuracy / Precision / Recall / RMSE / R¬≤\n",
    "\n",
    "Reflection: \n",
    "- Which features improved performance most? Why?\n",
    "- Observe bias-variance tradeoff\n",
    "\n",
    "Resource: [Feature Engineering Kaggle](https://www.kaggle.com/learn/feature-engineering)\n",
    "\n",
    "### Week 6 (Day 36‚Äì42) ‚Äì Fine-Tuning + Insights\n",
    "Tasks: \n",
    "- [ ] Tune hyperparameters (max depth, learning rate, n_estimators)\n",
    "- [ ] Compare all models and pick best performing\n",
    "- [ ] Create plots: feature importance, predicted vs actual\n",
    "- [ ] Write project summary: dataset, models, metrics, insights, lessons learned\n",
    "\n",
    "Reflection: \n",
    "- Connect feature importance to variance/correlation\n",
    "- Explain why best model performs well\n",
    "\n",
    "Resource: [Gradient Boosting Basics](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "\n",
    "---\n",
    "## Phase 3 ‚Äì Week 7‚Äì9: Advanced Project + Portfolio Prep\n",
    "### Week 7 (Day 43‚Äì49) ‚Äì Gradient Boosting Fundamentals\n",
    "Tasks: \n",
    "- [ ] Learn gradient boosting intuition\n",
    "- [ ] Apply GradientBoostingClassifier / Regressor on your dataset\n",
    "- [ ] Tune basic hyperparameters\n",
    "- [ ] Compare performance with previous models\n",
    "\n",
    "Reflection: \n",
    "- Why does boosting reduce bias?\n",
    "- How do weak learners combine to make strong predictions?\n",
    "\n",
    "Resource: [Gradient Boosting scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "\n",
    "### Week 8 (Day 50‚Äì56) ‚Äì Feature Engineering Mastery\n",
    "Tasks: \n",
    "- [ ] Advanced feature engineering (interactions, ratios, scaling, categorical encoding)\n",
    "- [ ] Retrain all models\n",
    "- [ ] Evaluate performance improvement\n",
    "\n",
    "Reflection: \n",
    "- Which engineered features had strongest impact? Why?\n",
    "- Connect feature effects to bias/variance and real-world reasoning\n",
    "\n",
    "Resource: [Feature Engineering Kaggle](https://www.kaggle.com/learn/feature-engineering)\n",
    "\n",
    "### Week 9 (Day 57‚Äì63) ‚Äì Portfolio Polish & Presentation Prep\n",
    "Tasks: \n",
    "- [ ] Finalize best model\n",
    "- [ ] Create visualizations: feature importance, prediction vs actual, confusion matrix\n",
    "- [ ] Summarize project in notebook / markdown / optional slide deck:\n",
    "    - Problem statement\n",
    "    - Dataset\n",
    "    - Models trained\n",
    "    - Evaluation metrics\n",
    "    - insights + reflections\n",
    "\n",
    "Reflection: \n",
    "- Be ready to explain ‚Äúwhy model works‚Äù, feature importance, overfitting handling\n",
    "\n",
    "Resource: [Data Science Portfolio Guide](https://towardsdatascience.com/how-to-build-your-data-science-portfolio-4b9f95898227)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
